{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlMIBHgdwOaY",
        "outputId": "d0948a8c-77c3-4fa9-fdcc-906dd20f0fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.11/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (11.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F82No8sdwTdZ"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from collections import defaultdict\n",
        "from operator import neg\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_drugs_smiles = pd.read_csv('/content/drug_smiles.csv')\n",
        "\n",
        "DRUG_TO_INDX_DICT = {drug_id: indx for indx, drug_id in enumerate(df_drugs_smiles['drug_id'])}\n",
        "\n",
        "drug_id_mol_graph_tup = [(id, Chem.MolFromSmiles(smiles.strip())) for id, smiles in zip(df_drugs_smiles['drug_id'], df_drugs_smiles['smiles'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CJjUrSgwzOr"
      },
      "outputs": [],
      "source": [
        "# Gettings information and features of atoms\n",
        "ATOM_MAX_NUM = max([m[1].GetNumAtoms() for m in drug_id_mol_graph_tup])\n",
        "AVAILABLE_ATOM_SYMBOLS = list({a.GetSymbol() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)})\n",
        "AVAILABLE_ATOM_DEGREES = list({a.GetDegree() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)})\n",
        "AVAILABLE_ATOM_TOTAL_HS = list({a.GetTotalNumHs() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)})\n",
        "max_valence = max(a.GetImplicitValence() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup))\n",
        "max_valence = max(max_valence, 9)\n",
        "AVAILABLE_ATOM_VALENCE = np.arange(max_valence + 1)\n",
        "\n",
        "MAX_ATOM_FC = abs(max([a.GetFormalCharge() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)]))\n",
        "MAX_ATOM_FC = MAX_ATOM_FC if MAX_ATOM_FC else 0\n",
        "MAX_RADICAL_ELC = abs(max([a.GetNumRadicalElectrons() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)]))\n",
        "MAX_RADICAL_ELC = MAX_RADICAL_ELC if MAX_RADICAL_ELC else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMkvpt3rw4m3",
        "outputId": "bce9eaef-ee83-418b-b126-fce912d2de28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-01d4583bbe8b>:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
            "  return undirected_edge_list.T, features\n"
          ]
        }
      ],
      "source": [
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def atom_features(atom,\n",
        "                explicit_H=True,\n",
        "                use_chirality=False):\n",
        "\n",
        "    results = one_of_k_encoding_unk(\n",
        "        atom.GetSymbol(),\n",
        "        ['C','N','O', 'S','F','Si','P', 'Cl','Br','Mg','Na','Ca','Fe','As','Al','I','B','V','K','Tl',\n",
        "            'Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn','H', 'Li','Ge','Cu','Au','Ni','Cd','In',\n",
        "            'Mn','Zr','Cr','Pt','Hg','Pb','Unknown'\n",
        "        ]) + [atom.GetDegree()/10, atom.GetImplicitValence(),\n",
        "                atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \\\n",
        "                one_of_k_encoding_unk(atom.GetHybridization(), [\n",
        "                Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
        "                Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.\n",
        "                                    SP3D, Chem.rdchem.HybridizationType.SP3D2\n",
        "                ]) + [atom.GetIsAromatic()]\n",
        "    # In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`\n",
        "    if explicit_H:\n",
        "        results = results + [atom.GetTotalNumHs()]\n",
        "\n",
        "    if use_chirality:\n",
        "        try:\n",
        "            results = results + one_of_k_encoding_unk(\n",
        "            atom.GetProp('_CIPCode'),\n",
        "            ['R', 'S']) + [atom.HasProp('_ChiralityPossible')]\n",
        "        except:\n",
        "            results = results + [False, False\n",
        "                            ] + [atom.HasProp('_ChiralityPossible')]\n",
        "\n",
        "    results = np.array(results).astype(np.float32)\n",
        "\n",
        "    return torch.from_numpy(results)\n",
        "\n",
        "\n",
        "def get_atom_features(atom, mode='one_hot'):\n",
        "\n",
        "    if mode == 'one_hot':\n",
        "        atom_feature = torch.cat([\n",
        "            one_of_k_encoding_unk(atom.GetSymbol(), AVAILABLE_ATOM_SYMBOLS),\n",
        "            one_of_k_encoding_unk(atom.GetDegree(), AVAILABLE_ATOM_DEGREES),\n",
        "            one_of_k_encoding_unk(atom.GetTotalNumHs(), AVAILABLE_ATOM_TOTAL_HS),\n",
        "            one_of_k_encoding_unk(atom.GetImplicitValence(), AVAILABLE_ATOM_VALENCE),\n",
        "            torch.tensor([atom.GetIsAromatic()], dtype=torch.float)\n",
        "        ])\n",
        "    else:\n",
        "        atom_feature = torch.cat([\n",
        "            one_of_k_encoding_unk(atom.GetSymbol(), AVAILABLE_ATOM_SYMBOLS),\n",
        "            torch.tensor([atom.GetDegree()]).float(),\n",
        "            torch.tensor([atom.GetTotalNumHs()]).float(),\n",
        "            torch.tensor([atom.GetImplicitValence()]).float(),\n",
        "            torch.tensor([atom.GetIsAromatic()]).float()\n",
        "        ])\n",
        "\n",
        "    return atom_feature\n",
        "\n",
        "\n",
        "def get_mol_edge_list_and_feat_mtx(mol_graph):\n",
        "    features = [(atom.GetIdx(), atom_features(atom)) for atom in mol_graph.GetAtoms()]\n",
        "    features.sort() # to make sure that the feature matrix is aligned according to the idx of the atom\n",
        "    _, features = zip(*features)\n",
        "    features = torch.stack(features)\n",
        "\n",
        "    edge_list = torch.LongTensor([(b.GetBeginAtomIdx(), b.GetEndAtomIdx()) for b in mol_graph.GetBonds()])\n",
        "    undirected_edge_list = torch.cat([edge_list, edge_list[:, [1, 0]]], dim=0) if len(edge_list) else edge_list\n",
        "\n",
        "    return undirected_edge_list.T, features\n",
        "\n",
        "\n",
        "MOL_EDGE_LIST_FEAT_MTX = {drug_id: get_mol_edge_list_and_feat_mtx(mol)\n",
        "                                for drug_id, mol in drug_id_mol_graph_tup}\n",
        "MOL_EDGE_LIST_FEAT_MTX = {drug_id: mol for drug_id, mol in MOL_EDGE_LIST_FEAT_MTX.items() if mol is not None}\n",
        "\n",
        "TOTAL_ATOM_FEATS = (next(iter(MOL_EDGE_LIST_FEAT_MTX.values()))[1].shape[-1])\n",
        "\n",
        "\n",
        "##### DDI statistics and counting #######\n",
        "df_all_pos_ddi = pd.read_csv('/content/ddis.csv')\n",
        "all_pos_tup = [(h, t, r) for h, t, r in zip(df_all_pos_ddi['d1'], df_all_pos_ddi['d2'], df_all_pos_ddi['type'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMkP6liDxC8x"
      },
      "outputs": [],
      "source": [
        "ALL_DRUG_IDS, _ = zip(*drug_id_mol_graph_tup)\n",
        "ALL_DRUG_IDS = np.array(list(set(ALL_DRUG_IDS)))\n",
        "ALL_TRUE_H_WITH_TR = defaultdict(list)\n",
        "ALL_TRUE_T_WITH_HR = defaultdict(list)\n",
        "\n",
        "FREQ_REL = defaultdict(int)\n",
        "ALL_H_WITH_R = defaultdict(dict)\n",
        "ALL_T_WITH_R = defaultdict(dict)\n",
        "ALL_TAIL_PER_HEAD = {}\n",
        "ALL_HEAD_PER_TAIL = {}\n",
        "\n",
        "for h, t, r in all_pos_tup:\n",
        "    ALL_TRUE_H_WITH_TR[(t, r)].append(h)\n",
        "    ALL_TRUE_T_WITH_HR[(h, r)].append(t)\n",
        "    FREQ_REL[r] += 1.0\n",
        "    ALL_H_WITH_R[r][h] = 1\n",
        "    ALL_T_WITH_R[r][t] = 1\n",
        "\n",
        "for t, r in ALL_TRUE_H_WITH_TR:\n",
        "    ALL_TRUE_H_WITH_TR[(t, r)] = np.array(list(set(ALL_TRUE_H_WITH_TR[(t, r)])))\n",
        "for h, r in ALL_TRUE_T_WITH_HR:\n",
        "    ALL_TRUE_T_WITH_HR[(h, r)] = np.array(list(set(ALL_TRUE_T_WITH_HR[(h, r)])))\n",
        "\n",
        "for r in FREQ_REL:\n",
        "    ALL_H_WITH_R[r] = np.array(list(ALL_H_WITH_R[r].keys()))\n",
        "    ALL_T_WITH_R[r] = np.array(list(ALL_T_WITH_R[r].keys()))\n",
        "    ALL_HEAD_PER_TAIL[r] = FREQ_REL[r] / len(ALL_T_WITH_R[r])\n",
        "    ALL_TAIL_PER_HEAD[r] = FREQ_REL[r] / len(ALL_H_WITH_R[r])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2FaO-7TxT90"
      },
      "outputs": [],
      "source": [
        "class DrugDataset(Dataset):\n",
        "    def __init__(self, tri_list, ratio=1.0,  neg_ent=1, disjoint_split=True, shuffle=True):\n",
        "        ''''disjoint_split: Consider whether entities should appear in one and only one split of the dataset\n",
        "        '''\n",
        "        self.neg_ent = neg_ent\n",
        "        self.tri_list = []\n",
        "        self.ratio = ratio\n",
        "\n",
        "        for h, t, r, *_ in tri_list:\n",
        "            if ((h in MOL_EDGE_LIST_FEAT_MTX) and (t in MOL_EDGE_LIST_FEAT_MTX)):\n",
        "                self.tri_list.append((h, t, r))\n",
        "\n",
        "        if disjoint_split:\n",
        "            d1, d2, *_ = zip(*self.tri_list)\n",
        "            self.drug_ids = np.array(list(set(d1 + d2)))\n",
        "        else:\n",
        "            self.drug_ids = ALL_DRUG_IDS\n",
        "\n",
        "        self.drug_ids = np.array([id for id in self.drug_ids if id in MOL_EDGE_LIST_FEAT_MTX])\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(self.tri_list)\n",
        "        limit = math.ceil(len(self.tri_list) * ratio)\n",
        "        self.tri_list = self.tri_list[:limit]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tri_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.tri_list[index]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        pos_rels = []\n",
        "        pos_h_samples = []\n",
        "        pos_t_samples = []\n",
        "        neg_rels = []\n",
        "        neg_h_samples = []\n",
        "        neg_t_samples = []\n",
        "\n",
        "        for h, t, r in batch:\n",
        "            pos_rels.append(r)\n",
        "            h_data = self.__create_graph_data(h)\n",
        "            t_data = self.__create_graph_data(t)\n",
        "            pos_h_samples.append(h_data)\n",
        "            pos_t_samples.append(t_data)\n",
        "\n",
        "            neg_heads, neg_tails = self.__normal_batch(h, t, r, self.neg_ent)\n",
        "\n",
        "            for neg_h in neg_heads:\n",
        "                neg_rels.append(r)\n",
        "                neg_h_samples.append(self.__create_graph_data(neg_h))\n",
        "                neg_t_samples.append(t_data)\n",
        "\n",
        "            for neg_t in neg_tails:\n",
        "                neg_rels.append(r)\n",
        "                neg_h_samples.append(h_data)\n",
        "                neg_t_samples.append(self.__create_graph_data(neg_t))\n",
        "\n",
        "        pos_h_samples = Batch.from_data_list(pos_h_samples)\n",
        "        pos_t_samples = Batch.from_data_list(pos_t_samples)\n",
        "        pos_rels = torch.LongTensor(pos_rels)\n",
        "        pos_tri = (pos_h_samples, pos_t_samples, pos_rels)\n",
        "\n",
        "        neg_h_samples = Batch.from_data_list(neg_h_samples)\n",
        "        neg_t_samples = Batch.from_data_list(neg_t_samples)\n",
        "        neg_rels = torch.LongTensor(neg_rels)\n",
        "        neg_tri = (neg_h_samples, neg_t_samples, neg_rels)\n",
        "\n",
        "        return pos_tri, neg_tri\n",
        "\n",
        "    def __create_graph_data(self, id):\n",
        "        edge_index = MOL_EDGE_LIST_FEAT_MTX[id][0]\n",
        "        features = MOL_EDGE_LIST_FEAT_MTX[id][1]\n",
        "\n",
        "        return Data(x=features, edge_index=edge_index)\n",
        "    def __corrupt_ent(self, other_ent, r, other_ent_with_r_dict, max_num=1):\n",
        "        corrupted_ents = []\n",
        "        current_size = 0\n",
        "        while current_size < max_num:\n",
        "            candidates = np.random.choice(self.drug_ids, (max_num - current_size) * 2)\n",
        "            mask = np.isin(candidates, other_ent_with_r_dict[(other_ent, r)], assume_unique=True, invert=True)\n",
        "            corrupted_ents.append(candidates[mask])\n",
        "            current_size += len(corrupted_ents[-1])\n",
        "\n",
        "        if corrupted_ents != []:\n",
        "            corrupted_ents = np.concatenate(corrupted_ents)\n",
        "\n",
        "        return np.asarray(corrupted_ents[:max_num])\n",
        "\n",
        "    def __corrupt_head(self, t, r, n=1):\n",
        "        return self.__corrupt_ent(t, r, ALL_TRUE_H_WITH_TR, n)\n",
        "\n",
        "    def __corrupt_tail(self, h, r, n=1):\n",
        "        return self.__corrupt_ent(h, r, ALL_TRUE_T_WITH_HR, n)\n",
        "\n",
        "    def __normal_batch(self, h, t, r, neg_size):\n",
        "        neg_size_h = 0\n",
        "        neg_size_t = 0\n",
        "        prob = ALL_TAIL_PER_HEAD[r] / (ALL_TAIL_PER_HEAD[r] + ALL_HEAD_PER_TAIL[r])\n",
        "        for i in range(neg_size):\n",
        "            if random.random() < prob:\n",
        "                neg_size_h += 1\n",
        "            else:\n",
        "                neg_size_t +=1\n",
        "\n",
        "        return (self.__corrupt_head(t, r, neg_size_h),\n",
        "                self.__corrupt_tail(h, r, neg_size_t))\n",
        "\n",
        "\n",
        "class DrugDataLoader(DataLoader):\n",
        "    def __init__(self, data, **kwargs):\n",
        "        super().__init__(data, collate_fn=data.collate_fn, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nm1jb94xjt0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SigmoidLoss(nn.Module):\n",
        "    def __init__(self, adv_temperature=None):\n",
        "        super().__init__()\n",
        "        self.adv_temperature = adv_temperature\n",
        "\n",
        "    def forward(self, p_scores, n_scores):\n",
        "        if self.adv_temperature:\n",
        "            weights= F.softmax(self.adv_temperature * n_scores, dim=-1).detach()\n",
        "            n_scores = weights * n_scores\n",
        "        p_loss = - F.logsigmoid(p_scores).mean()\n",
        "        n_loss = - F.logsigmoid(-n_scores).mean()\n",
        "\n",
        "        return (p_loss + n_loss) / 2, p_loss, n_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLMQz0EZxnA4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CoAttentionLayer(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.w_q = nn.Parameter(torch.zeros(n_features, n_features//2))\n",
        "        self.w_k = nn.Parameter(torch.zeros(n_features, n_features//2))\n",
        "        self.bias = nn.Parameter(torch.zeros(n_features // 2))\n",
        "        self.a = nn.Parameter(torch.zeros(n_features//2))\n",
        "\n",
        "        nn.init.xavier_uniform_(self.w_q)\n",
        "        nn.init.xavier_uniform_(self.w_k)\n",
        "        nn.init.xavier_uniform_(self.bias.view(*self.bias.shape, -1))\n",
        "        nn.init.xavier_uniform_(self.a.view(*self.a.shape, -1))\n",
        "\n",
        "    def forward(self, receiver, attendant):\n",
        "        keys = receiver @ self.w_k\n",
        "        queries = attendant @ self.w_q\n",
        "        # values = receiver @ self.w_v\n",
        "        values = receiver\n",
        "\n",
        "        e_activations = queries.unsqueeze(-3) + keys.unsqueeze(-2) + self.bias\n",
        "        e_scores = torch.tanh(e_activations) @ self.a\n",
        "        # e_scores = e_activations @ self.a\n",
        "        attentions = e_scores\n",
        "\n",
        "        return attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUDLZqt4xpxc"
      },
      "outputs": [],
      "source": [
        "class RESCAL(nn.Module):\n",
        "    def __init__(self, n_rels, n_features):\n",
        "        super().__init__()\n",
        "        self.n_rels = n_rels\n",
        "        self.n_features = n_features\n",
        "        self.rel_emb = nn.Embedding(self.n_rels, n_features * n_features)\n",
        "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
        "\n",
        "    def forward(self, heads, tails, rels, alpha_scores):\n",
        "        rels = self.rel_emb(rels)\n",
        "        rels = F.normalize(rels, dim=-1)\n",
        "        heads = F.normalize(heads, dim=-1)\n",
        "        tails = F.normalize(tails, dim=-1)\n",
        "        rels = rels.view(-1, self.n_features, self.n_features)\n",
        "\n",
        "        scores = heads @ rels @ tails.transpose(-2, -1)\n",
        "\n",
        "        if alpha_scores is not None:\n",
        "          scores = alpha_scores * scores\n",
        "        scores = scores.sum(dim=(-2, -1))\n",
        "        return scores\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.n_rels}, {self.rel_emb.weight.shape})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIoNNyguxvnl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.container import ModuleList\n",
        "from torch_geometric.nn import (GATConv,\n",
        "                                SAGPooling,\n",
        "                                LayerNorm,\n",
        "                                global_mean_pool,\n",
        "                                max_pool_neighbor_x,\n",
        "                                global_add_pool)\n",
        "\n",
        "\n",
        "\n",
        "class SSI_DDI(nn.Module):\n",
        "    def __init__(self, in_features, hidd_dim, kge_dim, rel_total, heads_out_feat_params, blocks_params):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidd_dim = hidd_dim\n",
        "        self.rel_total = rel_total\n",
        "        self.kge_dim = kge_dim\n",
        "        self.n_blocks = len(blocks_params)\n",
        "\n",
        "        self.initial_norm = LayerNorm(self.in_features)\n",
        "        self.blocks = []\n",
        "        self.net_norms = ModuleList()\n",
        "        for i, (head_out_feats, n_heads) in enumerate(zip(heads_out_feat_params, blocks_params)):\n",
        "            block = SSI_DDI_Block(n_heads, in_features, head_out_feats, final_out_feats=self.hidd_dim)\n",
        "            self.add_module(f\"block{i}\", block)\n",
        "            self.blocks.append(block)\n",
        "            self.net_norms.append(LayerNorm(head_out_feats * n_heads))\n",
        "            in_features = head_out_feats * n_heads\n",
        "\n",
        "        self.co_attention = CoAttentionLayer(self.kge_dim)\n",
        "        self.KGE = RESCAL(self.rel_total, self.kge_dim)\n",
        "\n",
        "    def forward(self, triples):\n",
        "        h_data, t_data, rels = triples\n",
        "\n",
        "        h_data.x = self.initial_norm(h_data.x, h_data.batch)\n",
        "        t_data.x = self.initial_norm(t_data.x, t_data.batch)\n",
        "\n",
        "        repr_h = []\n",
        "        repr_t = []\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            out1, out2 = block(h_data), block(t_data)\n",
        "\n",
        "            h_data = out1[0]\n",
        "            t_data = out2[0]\n",
        "            r_h = out1[1]\n",
        "            r_t = out2[1]\n",
        "\n",
        "            repr_h.append(r_h)\n",
        "            repr_t.append(r_t)\n",
        "\n",
        "            h_data.x = F.elu(self.net_norms[i](h_data.x, h_data.batch))\n",
        "            t_data.x = F.elu(self.net_norms[i](t_data.x, t_data.batch))\n",
        "\n",
        "        repr_h = torch.stack(repr_h, dim=-2)\n",
        "        repr_t = torch.stack(repr_t, dim=-2)\n",
        "\n",
        "        kge_heads = repr_h\n",
        "        kge_tails = repr_t\n",
        "\n",
        "        attentions = self.co_attention(kge_heads, kge_tails)\n",
        "        # attentions = None\n",
        "        scores = self.KGE(kge_heads, kge_tails, rels, attentions)\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "class SSI_DDI_Block(nn.Module):\n",
        "    def __init__(self, n_heads, in_features, head_out_feats, final_out_feats):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.in_features = in_features\n",
        "        self.out_features = head_out_feats\n",
        "        self.conv = GATConv(in_features, head_out_feats, n_heads)\n",
        "        self.readout = SAGPooling(n_heads * head_out_feats, min_score=-1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        data.x = self.conv(data.x, data.edge_index)\n",
        "        att_x, att_edge_index, att_edge_attr, att_batch, att_perm, att_scores= self.readout(data.x, data.edge_index, batch=data.batch)\n",
        "        global_graph_emb = global_add_pool(att_x, att_batch)\n",
        "\n",
        "        # data = max_pool_neighbor_x(data)\n",
        "        return data, global_graph_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM97wK5Uxvjg"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGFQyLUvx1jh"
      },
      "outputs": [],
      "source": [
        "df_ddi_train = pd.read_csv('/content/ddi_training.csv')\n",
        "df_ddi_val = pd.read_csv('/content/ddi_validation.csv')\n",
        "df_ddi_test = pd.read_csv('/content/ddi_test.csv')\n",
        "\n",
        "\n",
        "train_tup = [(h, t, r) for h, t, r in zip(df_ddi_train['d1'], df_ddi_train['d2'], df_ddi_train['type'])]\n",
        "val_tup = [(h, t, r) for h, t, r in zip(df_ddi_val['d1'], df_ddi_val['d2'], df_ddi_val['type'])]\n",
        "test_tup = [(h, t, r) for h, t, r in zip(df_ddi_test['d1'], df_ddi_test['d2'], df_ddi_test['type'])]\n",
        "\n",
        "\n",
        "total = len(val_tup) + len(train_tup) + len(test_tup)\n",
        "len(train_tup) / total, len(test_tup)/total, len(val_tup)/total\n",
        "# Hyperparameters\n",
        "n_atom_feats = TOTAL_ATOM_FEATS\n",
        "n_atom_hid = 64\n",
        "rel_total = 86\n",
        "lr = 1e-2\n",
        "weight_decay = 5e-4\n",
        "n_epochs = 300\n",
        "neg_samples = 1\n",
        "batch_size = 1024\n",
        "data_size_ratio = 1\n",
        "kge_dim = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehubm5_c19vC",
        "outputId": "7180299e-f9f9-497f-e908-c2317c42acf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with 115185 samples, validating with 38348, and testing with 38337\n"
          ]
        }
      ],
      "source": [
        "train_data = DrugDataset(train_tup, ratio=data_size_ratio, neg_ent=neg_samples)\n",
        "val_data = DrugDataset(val_tup, ratio=data_size_ratio, disjoint_split=False)\n",
        "test_data = DrugDataset(test_tup, disjoint_split=False)\n",
        "print(f\"Training with {len(train_data)} samples, validating with {len(val_data)}, and testing with {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36aey71h2Yh-"
      },
      "outputs": [],
      "source": [
        "train_data_loader = DrugDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_data_loader = DrugDataLoader(val_data, batch_size=batch_size *3)\n",
        "test_data_loader = DrugDataLoader(test_data, batch_size=batch_size *3)\n",
        "def do_compute(batch, device, training=True):\n",
        "        '''\n",
        "            *batch: (pos_tri, neg_tri)\n",
        "            *pos/neg_tri: (batch_h, batch_t, batch_r)\n",
        "        '''\n",
        "        probas_pred, ground_truth = [], []\n",
        "        pos_tri, neg_tri = batch\n",
        "\n",
        "        pos_tri = [tensor.to(device=device) for tensor in pos_tri]\n",
        "        p_score = model(pos_tri)\n",
        "        probas_pred.append(torch.sigmoid(p_score.detach()).cpu())\n",
        "        ground_truth.append(np.ones(len(p_score)))\n",
        "\n",
        "        neg_tri = [tensor.to(device=device) for tensor in neg_tri]\n",
        "        n_score = model(neg_tri)\n",
        "        probas_pred.append(torch.sigmoid(n_score.detach()).cpu())\n",
        "        ground_truth.append(np.zeros(len(n_score)))\n",
        "\n",
        "        probas_pred = np.concatenate(probas_pred)\n",
        "        ground_truth = np.concatenate(ground_truth)\n",
        "\n",
        "        return p_score, n_score, probas_pred, ground_truth\n",
        "def do_compute_metrics(probas_pred, target):\n",
        "\n",
        "    pred = (probas_pred >= 0.5).astype(int)\n",
        "\n",
        "    acc = metrics.accuracy_score(target, pred)\n",
        "    auc_roc = metrics.roc_auc_score(target, probas_pred)\n",
        "    f1_score = metrics.f1_score(target, pred)\n",
        "\n",
        "    p, r, t = metrics.precision_recall_curve(target, probas_pred)\n",
        "    auc_prc = metrics.auc(r, p)\n",
        "\n",
        "    return acc, auc_roc, auc_prc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz_WOGYq2fk3"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data_loader, val_data_loader, loss_fn,  optimizer, n_epochs, device, scheduler=None):\n",
        "    print('Starting training at', datetime.today())\n",
        "    for i in range(1, n_epochs+1):\n",
        "        train_loss = 0\n",
        "        train_loss_pos = 0\n",
        "        train_loss_neg = 0\n",
        "        val_loss = 0\n",
        "        val_loss_pos = 0\n",
        "        val_loss_neg = 0\n",
        "        train_probas_pred = []\n",
        "        train_ground_truth = []\n",
        "        val_probas_pred = []\n",
        "        val_ground_truth = []\n",
        "\n",
        "        for batch in train_data_loader:\n",
        "            model.train()\n",
        "            p_score, n_score, probas_pred, ground_truth = do_compute(batch, device)\n",
        "            train_probas_pred.append(probas_pred)\n",
        "            train_ground_truth.append(ground_truth)\n",
        "            loss, loss_p, loss_n = loss_fn(p_score, n_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * len(p_score)\n",
        "        train_loss /= len(train_data)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            train_probas_pred = np.concatenate(train_probas_pred)\n",
        "            train_ground_truth = np.concatenate(train_ground_truth)\n",
        "\n",
        "            train_acc, train_auc_roc, train_auc_prc = do_compute_metrics(train_probas_pred, train_ground_truth)\n",
        "\n",
        "            for batch in val_data_loader:\n",
        "                model.eval()\n",
        "                p_score, n_score, probas_pred, ground_truth = do_compute(batch, device)\n",
        "                val_probas_pred.append(probas_pred)\n",
        "                val_ground_truth.append(ground_truth)\n",
        "                loss, loss_p, loss_n = loss_fn(p_score, n_score)\n",
        "                val_loss += loss.item() * len(p_score)\n",
        "\n",
        "            val_loss /= len(val_data)\n",
        "            val_probas_pred = np.concatenate(val_probas_pred)\n",
        "            val_ground_truth = np.concatenate(val_ground_truth)\n",
        "            val_acc, val_auc_roc, val_auc_prc = do_compute_metrics(val_probas_pred, val_ground_truth)\n",
        "\n",
        "        if scheduler:\n",
        "            print('scheduling')\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "        print(f'Epoch: {i} (train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f},'\n",
        "        f' train_acc: {train_acc:.4f}, val_acc:{val_acc:.4f}')\n",
        "        print(f'\\t\\ttrain_roc: {train_auc_roc:.4f}, val_roc: {val_auc_roc:.4f}, train_auprc: {train_auc_prc:.4f}, val_auprc: {val_auc_prc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgKxQTI32hpx",
        "outputId": "995c8d7f-3cbd-43d4-8259-fb04e9c2257e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SSI_DDI(\n",
              "  (initial_norm): LayerNorm(55, affine=True, mode=graph)\n",
              "  (net_norms): ModuleList(\n",
              "    (0-3): 4 x LayerNorm(64, affine=True, mode=graph)\n",
              "  )\n",
              "  (block0): SSI_DDI_Block(\n",
              "    (conv): GATConv(55, 32, heads=2)\n",
              "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
              "  )\n",
              "  (block1): SSI_DDI_Block(\n",
              "    (conv): GATConv(64, 32, heads=2)\n",
              "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
              "  )\n",
              "  (block2): SSI_DDI_Block(\n",
              "    (conv): GATConv(64, 32, heads=2)\n",
              "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
              "  )\n",
              "  (block3): SSI_DDI_Block(\n",
              "    (conv): GATConv(64, 32, heads=2)\n",
              "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
              "  )\n",
              "  (co_attention): CoAttentionLayer()\n",
              "  (KGE): RESCAL(86, torch.Size([86, 4096]))\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SSI_DDI(n_atom_feats, n_atom_hid, kge_dim, rel_total, heads_out_feat_params=[32, 32, 32, 32], blocks_params=[2, 2, 2, 2])\n",
        "loss = SigmoidLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.96 ** (epoch))\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "czXztI4T2k-b",
        "outputId": "5736ef9b-c06e-4173-8283-141dc4af87c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training at 2025-02-04 12:53:38.923386\n",
            "scheduling\n",
            "Epoch: 1 (train_loss: 0.6415, val_loss: 0.5945, train_acc: 0.6140, val_acc:0.6727\n",
            "\t\ttrain_roc: 0.6695, val_roc: 0.7392, train_auprc: 0.6566, val_auprc: 0.7149\n",
            "scheduling\n",
            "Epoch: 2 (train_loss: 0.5707, val_loss: 0.5527, train_acc: 0.6941, val_acc:0.7097\n",
            "\t\ttrain_roc: 0.7659, val_roc: 0.7850, train_auprc: 0.7464, val_auprc: 0.7664\n",
            "scheduling\n",
            "Epoch: 3 (train_loss: 0.5394, val_loss: 0.5337, train_acc: 0.7215, val_acc:0.7260\n",
            "\t\ttrain_roc: 0.7973, val_roc: 0.8033, train_auprc: 0.7768, val_auprc: 0.7825\n",
            "scheduling\n",
            "Epoch: 4 (train_loss: 0.5229, val_loss: 0.5185, train_acc: 0.7349, val_acc:0.7405\n",
            "\t\ttrain_roc: 0.8125, val_roc: 0.8167, train_auprc: 0.7929, val_auprc: 0.7972\n",
            "scheduling\n",
            "Epoch: 5 (train_loss: 0.5110, val_loss: 0.5100, train_acc: 0.7443, val_acc:0.7466\n",
            "\t\ttrain_roc: 0.8227, val_roc: 0.8248, train_auprc: 0.8027, val_auprc: 0.8062\n",
            "scheduling\n",
            "Epoch: 6 (train_loss: 0.5005, val_loss: 0.4969, train_acc: 0.7522, val_acc:0.7561\n",
            "\t\ttrain_roc: 0.8310, val_roc: 0.8350, train_auprc: 0.8109, val_auprc: 0.8154\n",
            "scheduling\n",
            "Epoch: 7 (train_loss: 0.4936, val_loss: 0.4910, train_acc: 0.7580, val_acc:0.7597\n",
            "\t\ttrain_roc: 0.8366, val_roc: 0.8388, train_auprc: 0.8157, val_auprc: 0.8210\n",
            "scheduling\n",
            "Epoch: 8 (train_loss: 0.4850, val_loss: 0.4856, train_acc: 0.7630, val_acc:0.7592\n",
            "\t\ttrain_roc: 0.8430, val_roc: 0.8426, train_auprc: 0.8224, val_auprc: 0.8244\n",
            "scheduling\n",
            "Epoch: 9 (train_loss: 0.4802, val_loss: 0.4776, train_acc: 0.7676, val_acc:0.7704\n",
            "\t\ttrain_roc: 0.8465, val_roc: 0.8494, train_auprc: 0.8261, val_auprc: 0.8295\n",
            "scheduling\n",
            "Epoch: 10 (train_loss: 0.4718, val_loss: 0.4685, train_acc: 0.7742, val_acc:0.7754\n",
            "\t\ttrain_roc: 0.8526, val_roc: 0.8562, train_auprc: 0.8316, val_auprc: 0.8377\n",
            "scheduling\n",
            "Epoch: 11 (train_loss: 0.4650, val_loss: 0.4711, train_acc: 0.7791, val_acc:0.7717\n",
            "\t\ttrain_roc: 0.8576, val_roc: 0.8561, train_auprc: 0.8373, val_auprc: 0.8357\n",
            "scheduling\n",
            "Epoch: 12 (train_loss: 0.4604, val_loss: 0.4621, train_acc: 0.7826, val_acc:0.7815\n",
            "\t\ttrain_roc: 0.8604, val_roc: 0.8597, train_auprc: 0.8393, val_auprc: 0.8389\n",
            "scheduling\n",
            "Epoch: 13 (train_loss: 0.4530, val_loss: 0.4602, train_acc: 0.7866, val_acc:0.7814\n",
            "\t\ttrain_roc: 0.8654, val_roc: 0.8610, train_auprc: 0.8455, val_auprc: 0.8404\n",
            "scheduling\n",
            "Epoch: 14 (train_loss: 0.4505, val_loss: 0.4539, train_acc: 0.7883, val_acc:0.7877\n",
            "\t\ttrain_roc: 0.8672, val_roc: 0.8655, train_auprc: 0.8476, val_auprc: 0.8457\n",
            "scheduling\n",
            "Epoch: 15 (train_loss: 0.4449, val_loss: 0.4456, train_acc: 0.7925, val_acc:0.7933\n",
            "\t\ttrain_roc: 0.8712, val_roc: 0.8712, train_auprc: 0.8514, val_auprc: 0.8534\n",
            "scheduling\n",
            "Epoch: 16 (train_loss: 0.4387, val_loss: 0.4478, train_acc: 0.7967, val_acc:0.7925\n",
            "\t\ttrain_roc: 0.8747, val_roc: 0.8711, train_auprc: 0.8549, val_auprc: 0.8535\n",
            "scheduling\n",
            "Epoch: 17 (train_loss: 0.4334, val_loss: 0.4350, train_acc: 0.7996, val_acc:0.8009\n",
            "\t\ttrain_roc: 0.8782, val_roc: 0.8772, train_auprc: 0.8595, val_auprc: 0.8574\n",
            "scheduling\n",
            "Epoch: 18 (train_loss: 0.4326, val_loss: 0.4412, train_acc: 0.8007, val_acc:0.7967\n",
            "\t\ttrain_roc: 0.8783, val_roc: 0.8737, train_auprc: 0.8585, val_auprc: 0.8547\n",
            "scheduling\n",
            "Epoch: 19 (train_loss: 0.4240, val_loss: 0.4310, train_acc: 0.8061, val_acc:0.8015\n",
            "\t\ttrain_roc: 0.8840, val_roc: 0.8796, train_auprc: 0.8655, val_auprc: 0.8614\n"
          ]
        }
      ],
      "source": [
        "model.to(device=device);\n",
        "train(model, train_data_loader, val_data_loader, loss, optimizer, n_epochs, device, scheduler)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}